# 主成分分析

## 基本介绍

PCA principal component analysis：主成分分析；

这个方法常用于：**对高维数据的降维，可以提取数据的主要特征分量**

在介绍PCA之前，我们需要了解（复习）一些基本的线性代数知识

### 基变换

假设在基向量$\begin{bmatrix}0&1\end{bmatrix}$和$\begin{bmatrix}1&0\end{bmatrix}$定义的空间中（即最常见的x-y坐标轴平面）有两个向量$\begin{bmatrix}1\\1 \end{bmatrix}$和$\begin{bmatrix}2\\2 \end{bmatrix}$;

如果我们将这两个向量表示到基向量为$\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{bmatrix}$和$\begin{bmatrix}-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{bmatrix}$的空间中会发现

向量$\begin{bmatrix}1\\1 \end{bmatrix}$->$\begin{bmatrix}\sqrt{2}\\0 \end{bmatrix}$

向量$\begin{bmatrix} 2\\2 \end{bmatrix}$->$\begin{bmatrix} \sqrt{8} \\0 \end{bmatrix}$

**我们正在使用不同的”基准“去描述同一个向量**

这个过程可以使用**矩阵的乘法表示**

$\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}  \end{bmatrix} \begin{bmatrix} 1&2\\1&2\end{bmatrix} = \begin{bmatrix} \sqrt{2}&\sqrt{8}\\0&0 \end{bmatrix}$

即：将右边的矩阵中的列表示的向量->左侧矩阵行向量为基的空间中

*（实际上这个例子是一个比较特殊的情况...如果仔细观察会发现实际上起到了”基“的作用的，只有$\begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix}$，上面这个变换也可以被表述为，将原本的向量压缩到$\begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix}$为基的一维空间中）*



### 维度的压缩

在基变换中，左侧矩阵的行数等于右侧矩阵的列数时，变换后矩阵的维度可能会被压缩，但是如果左侧矩阵的行数小于右侧矩阵的列数，那么右侧矩阵一定会失去一些维度，例如

$\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\ 0 & 0  \end{bmatrix} \begin{bmatrix} 1&2\\1&2\end{bmatrix} = \begin{bmatrix} \sqrt{2}&\sqrt{8}\\0&0 \end{bmatrix}$

那么我们如何选择基，让右侧的信息能够被最大程度的保留？



### 面对一维数据，保证数据的"方差"大

在信息论中，我们认为熵越高，那么包含的信息就越多，熵越低，包含的信息就越少；

从上面的例子也可以看出；当我们使用$\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\ 0&0  \end{bmatrix} \begin{bmatrix} 1&2\\2&4\end{bmatrix} = \begin{bmatrix} \frac{3}{\sqrt{2}}&\frac{6}{\sqrt{2}}\\0&0 \end{bmatrix}$进行压缩的后，得到的结果还是离散的，此时我们已经无法再通过$\begin{bmatrix} \frac{3}{\sqrt{2}}&\frac{6}{\sqrt{2}}\\0&0 \end{bmatrix}$还原出$\begin{bmatrix} 1&2\\2&4\end{bmatrix}$，但是通过变换后的结果我们仍能获取一些信息，例如$\begin{bmatrix}\frac{6}{\sqrt{2}} \\0\end{bmatrix}\\$比$\begin{bmatrix}\frac{3}{\sqrt{2}} \\0\end{bmatrix}\\$长；

但如果假设变换是这样的：$\begin{bmatrix}2 & -1\\ 0&0  \end{bmatrix} \begin{bmatrix} 1&2\\2&4\end{bmatrix} = \begin{bmatrix}0&0\\0&0 \end{bmatrix}$，这个结果的熵就很低，所有结果都重叠到了一起，我们很难从这个变换结果中看到原本的向量所包含的信息；



**因此，尽管维度的压缩不逆，但是我们希望有效的信息不要被压缩的过多，我们希望变换后的结果尽可能的离散**



从这个角度看”方差“是一个很好的工具，因为方差描述的是数据间的离散程度；

所以**”如何将二维数据压缩到一维的同时，保留尽量多的信息“**等价于**”如何寻找一个基，在这个基变换下，结果的方差最大“**



### 面对高维数据，保证维度间的”协方差“小

如果需要将N维的数据压缩到K维上，除了需要让每个维度的方差尽可能大之外，还需要**让维度两两之间尽可能不相关**；**因为两个维度（变量）相关就意味着两个维度表达的信息有所重复**

协方差是用来描述两个变量之间的相关性的，协方差越大，相关性越强（协方差>0表示正相关，反之表示负相关）

**用向量来表示：正交向量不相关**



### 总结

我们降维的目的是**利用尽可能少的维度表示尽可能多的信息**

假设我们要将N维数据降到K维，要实现上述的要求，需要寻找K个相互正交的单位基，并且将数据映射到这K个单位基决定的空间后，每个基方向上的数据的方差还要尽可能大



## 霍特林变换

霍特林变化是站在方差角度上看效果最好的变换方法；PCA其实本身也是霍特林变换的一个应用；
