# 主成分分析

## 基本介绍

PCA principal component analysis：主成分分析；

这个方法常用于：**对高维数据的降维，可以提取数据的主要特征分量**

在介绍PCA之前，我们需要了解（复习）一些基本的线性代数知识

### 维度

**！！！要注意“维度”这个词在不同语境下有不同的涵义**

#### 矩阵的维度

$\begin{bmatrix}1&2&...&n \end{bmatrix}$是一维的，因为这个矩阵只有行自由度（同理，列向量也是一维的）

$\begin{bmatrix}1&2&...&n\\...&...&...&...\\1&2&...&n \end{bmatrix}$是二维的，因为这个矩阵拥有行自由度和列自由度；

#### “降维”所讨论的“维度”

在PCA中，如果给定一个M行N列的矩阵，我们通常认为，**行是特征，列是样本**；（当然组织数据的时候也应当组织成这个形式..）；在进行PCA时，有多少个特征，我们就称之为几维；

$\begin{bmatrix}1&1&...&n\\...&...&...&...\\n&n&...&n \end{bmatrix}$

同样是这个矩阵，如果对其进行PCA，我们称其维度为n（有n行，即有n个特征）



### 基变换

假设在基向量$\begin{bmatrix}0&1\end{bmatrix}$和$\begin{bmatrix}1&0\end{bmatrix}$定义的空间中（即最常见的x-y坐标轴平面）有两个向量$\begin{bmatrix}1\\1 \end{bmatrix}$和$\begin{bmatrix}2\\2 \end{bmatrix}$;

如果我们将这两个向量表示到基向量为$\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{bmatrix}$和$\begin{bmatrix}-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \end{bmatrix}$的空间中会发现

向量$\begin{bmatrix}1\\1 \end{bmatrix}$->$\begin{bmatrix}\sqrt{2}\\0 \end{bmatrix}$

向量$\begin{bmatrix} 2\\2 \end{bmatrix}$->$\begin{bmatrix} \sqrt{8} \\0 \end{bmatrix}$

**我们正在使用不同的”基准“去描述同一个向量**

这个过程可以使用**矩阵的乘法表示**

$\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\ -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}  \end{bmatrix} \begin{bmatrix} 1&2\\1&2\end{bmatrix} = \begin{bmatrix} \sqrt{2}&\sqrt{8}\\0&0 \end{bmatrix}$

即：将右边的矩阵中的列表示的向量->左侧矩阵行向量为基的空间中

*（实际上这个例子是一个比较特殊的情况...如果仔细观察会发现实际上起到了”基“的作用的，只有$\begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix}$，上面这个变换也可以被表述为，将原本的向量压缩到$\begin{bmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix}$为基的一维空间中）*



### 维度的压缩

在基变换中，左侧矩阵的行数等于右侧矩阵的列数时，变换后矩阵的维度可能会被压缩，但是如果左侧矩阵的行数小于右侧矩阵的列数，那么右侧矩阵一定会失去一些维度，例如

$\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} 1&2\\1&2\end{bmatrix} = \begin{bmatrix} \sqrt{2}&\sqrt{8}\end{bmatrix}$

那么我们如何选择基，让右侧的信息能够被最大程度的保留？



### 面对一维数据，保证数据的"方差"大

在信息论中，我们认为熵越高，那么包含的信息就越多，熵越低，包含的信息就越少；

从上面的例子也可以看出；当我们使用$\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} 1&2\\2&4\end{bmatrix} = \begin{bmatrix} \frac{3}{\sqrt{2}}&\frac{6}{\sqrt{2}}\end{bmatrix}$进行压缩的后，得到的结果还是离散的，此时我们已经无法再通过$\begin{bmatrix} \frac{3}{\sqrt{2}}&\frac{6}{\sqrt{2}}\end{bmatrix}$还原出$\begin{bmatrix} 1&2\\2&4\end{bmatrix}$，但是通过变换后的结果我们仍能获取一些信息，例如$\begin{bmatrix}\frac{6}{\sqrt{2}} \end{bmatrix}\\$比$\begin{bmatrix}\frac{3}{\sqrt{2}} \end{bmatrix}\\$长；

但如果假设变换是这样的：$\begin{bmatrix}2 & -1  \end{bmatrix} \begin{bmatrix} 1&2\\2&4\end{bmatrix} = \begin{bmatrix}0&0 \end{bmatrix}$，这个结果的熵就很低，所有两个向量都重叠到了一起，我们很难从这个变换结果中看到原本的向量所包含的信息；



**因此，维度的压缩不可逆，但我们希望有效的信息不要被压缩的过多，我们希望变换后的结果尽可能的离散**



从这个角度看”**方差**“是一个很好的工具，因为方差描述的是数据间的离散程度；

所以**”如何将二维数据压缩到一维的同时，保留尽量多的信息“**等价于**”如何寻找一个基，在这个基变换下，结果的方差最大“**



### 面对高维数据，保证维度间的”协方差“小

如果需要将N维的数据压缩到K维上，除了需要让每个维度的方差尽可能大之外，还需要**让维度两两之间尽可能不相关**；**因为两个维度（变量）相关就意味着两个维度表达的信息有所重复**

协方差是用来描述两个变量之间的相关性的，协方差越大，相关性越强（协方差>0表示正相关，反之表示负相关）

**用向量来表示：正交向量不相关**



### 总结

我们降维的目的是**利用尽可能少的维度表示尽可能多的信息**

假设我们要将N维数据降到K维，要实现上述的要求，就需要：

1. 寻找K个相互**正交**的单位基
2. 将数据映射到这K个单位基决定的空间后，每个基方向上的数据的方差还要尽可能大



## 霍特林变换

霍特林变化是站在方差角度上看效果最好的变换方法；PCA其实本身也是霍特林变换的一个应用；

### 霍特林变换的步骤

设：有一个M\*N的矩阵X：

1. 求出M\*N矩阵的协方差矩阵C
2. 求出协方差矩阵C的特征值和特征向量
3. 对特征值降序排序，取对应的前K个特征向量
4. 对特征向量进行单位正交化
5. 将特征向量（列向量）转置，选择K个特征向量，构造一个矩阵P
6. Y = PX；Y就是X降维到K维之后的结果

### 霍特林变换的原理

协方差矩阵在霍特林变换中有重要的意义，我们先了解一哈协方差矩阵；

#### 协方差矩阵

原始数据有M个维度，就可以构成M\*M的协方差矩阵；

设有如下矩阵$\begin{bmatrix}x_{11} & x_{12}&x_{13}\\x_{21}&x_{22}&x_{23} \end{bmatrix}$；

这个矩阵的维度是2

我们将下面这个矩阵称为协方差矩阵：$\begin{bmatrix}cov(x_1,x_1) & cov(x_1,x_2) \\cov(x_2,x_1)&cov(x_2,x_2) \end{bmatrix}$

很明显，$cov(x_1,x_1)$就是$x_1$的方差，同时$cov(x_1,x_2) == cov(x_2,x_1)$；

我们发现，协方差矩阵拥有很好的性质：

1. 它是一个实对称矩阵
2. 它的对角线是每个特征的方差



#### 推导

假设有如下集合$F=\{f_1,f_2,...,f_m\}$；每个$f_i$是一个特征，这个集合共有n个特征，m个样本

其中$f_i$是n\*1的列向量，即$f_1 = \begin{bmatrix} f_{11}\\f_{21}\\...\\f_{n1} \end{bmatrix}$

为了推导的方便我们假设给出的这个集合是经过均值化的（即$f_{11}+f_{21}+...+f_{n1} = 0$）

集合$F$看起来应该是这样的$F=\begin{bmatrix} \begin{bmatrix}f_{11}\\f_{21}\\...\\f_{n1} \end{bmatrix} &\begin{bmatrix}f_{12}\\f_{22}\\...\\f_{n2} \end{bmatrix}&...&\begin{bmatrix}f_{1m}\\f_{2m}\\...\\f_{nm} \end{bmatrix} \end{bmatrix}$；每一行是一个样本

集合$f$的协方差矩阵为$C_f = E\{FF^T\}= \frac{1}{n}FF^T$

**根据协方差矩阵的性质接下来我们需要将协方差对角线之外的元素（也就是协方差）全部变成0（协方差要小），然后根据对角线上的元素（也就是方差）的大小选择基向量（方差要尽量大）**

假设我们使用一个变换A完成了Y=AF，变换之后的Y的协方差矩阵满足上面的要求，此时有：

（**此时我们并不知道$Y，A，C_y$具体是多少；已知的条件是$C_y$是对角矩阵，现在的任务是求出A**）

$Y=AF$

$C_y = \frac{1}{n}YY^T$

$C_y = \frac{1}{n}[(AF)(AF)^T]$

$C_y = \frac{1}{n}[AFF^TA^T]$

由于$C_f = \frac{1}{n}FF^T$，代换进$C_y$

有$C_y = AC_fA^T$

根据要求，$C_y$应该是一个对角矩阵，那么**已知$C_f$要求$C_y$是对角矩阵，那么这个问题就是矩阵的对角化**

**注：矩阵的对角化不唯一，但不唯一的原因是对角化后的特征值可以随意排列，如果规定特征值从大到小，从左上角到右下角，那么对焦矩阵就是唯一的**

此时，问题转变为了求对角矩阵，此时A就可以求出来了；其实A就是$C_f$的特征值构成的矩阵



### 霍特林变换的过程中，信息的变化

1. **必须指出的是，霍特林变换之后，新生成的矩阵虽然保有旧矩阵的信息，但是新矩阵的特征已经不是旧矩阵的特征了；它更像是使用原有特征生成的新特征**
2. 对协方差矩阵而言，特征值越大，保有的信息越多，特征值越小，保有的信息越少；一般认为特征值很小的部分都是噪音，即使它不是噪音，它保有的信息也很少（**特征值就是方差**）



## MatLab中对PCA的实现

matlab中有一个原生函数实现PCA，它使用的是SVD，奇异值分解（所以霍特林变换并不是实现PCA的唯一方式）；

这个函数是princomp(x)，在2017b版本后更名为pca(x)

与我们上面举得例子不同的是，matlab选择使用行作为样本，列作为特征；因此取得的变换矩阵coeff在使用时不是coeff\*X而是X\*coeff



## 参考

https://zhuanlan.zhihu.com/p/77151308

